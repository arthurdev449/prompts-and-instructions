# Glossary of Prompt Engineering Terms

*   **Chain-of-Thought (CoT):** A prompting technique that guides the LLM's reasoning process by providing intermediate steps.
*   **Few-shot Prompting:** Providing the LLM with a few examples of the desired input-output pairs.
*   **Hallucination:**  When an LLM generates information that is factually incorrect or nonsensical.
*   **Large Language Model (LLM):** A type of AI model trained on a massive dataset of text and code, capable of generating human-quality text, translating languages, writing different kinds of creative content, and answering your questions in an informative way.
*   **Prompt:** The input text provided to an LLM to elicit a response.
*   **Prompt Engineering:** The art and science of crafting effective prompts to guide LLMs towards desired outputs.
*   **Retrieval-Augmented Generation (RAG):** A technique that enhances LLM responses by retrieving relevant information from an external knowledge source.  This involves searching an external database or document collection for information relevant to the prompt and then providing that information to the LLM, along with the original prompt.
*   **System Instructions:** Foundational instructions that configure the overall behavior and persona of the LLM.
*   **Temperature:** A parameter that controls the randomness of the LLM's output.  Lower temperatures result in more deterministic and predictable outputs, while higher temperatures lead to more creative and varied outputs.
*   **Token:** A basic unit of text or code that an LLM processes.  Tokens can be words, subwords, or characters.
*   **Top-p (Nucleus Sampling):** A parameter that controls the diversity of the LLM's output by limiting the selection of tokens to a subset with a cumulative probability above a certain threshold (p).
*   **Zero-shot Prompting:** Prompting the LLM to perform a task without providing any examples.
*   **Prompt Injection:** A type of security vulnerability where a malicious user crafts an input that manipulates the LLM's output, causing it to ignore previous instructions or generate unintended content.
*   **Delimiters:** Characters or sequences of characters used to separate different parts of a prompt (e.g., instructions, context, examples).
*   **Output Priming:** Providing the beginning of the desired output to guide the LLM's response.
*   **Context Window:** The maximum amount of text (measured in tokens) that an LLM can consider when generating a response.  This includes the prompt and the generated text.
*   **Fine-tuning:**  The process of further training a pre-trained LLM on a specific dataset to improve its performance on a particular task.
*   **Embedding:** A numerical representation of a piece of text, often used for tasks like similarity search and classification.
*   **Input:** Text provided to the LLM.
